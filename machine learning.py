# -*- coding: utf-8 -*-
"""MachineLearning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JlL-Ic4M98-fwZWrLnONPdgxTxt6gTkE
"""

# Setup
!pip install pandas numpy matplotlib seaborn --quiet

# Import required packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Display all columns and avoid scientific notation
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', lambda x: f'{x:.3f}')

print("Libraries imported successfully!")

# Load the Dataset
file_path = '/content/health_facility_locations (RAW).csv'

# Read CSV into a pandas DataFrame
df = pd.read_csv(file_path)

# Display dataset shape and a quick preview
print(f"Dataset loaded successfully with shape: {df.shape}")
display(df.head())

# Basic Data Exploration
# Check column names
print("\n Column Names:")
print(df.columns.tolist())

# Check data types summary
print("\n Data Types:")
print(df.dtypes.value_counts())

# Count missing values (top 15 columns)
print("\n Missing Values (Top 15):")
print(df.isna().sum().sort_values(ascending=False).head(15))

# Quick statistical summary (for numeric columns)
print("\n Summary Statistics (Numeric Columns):")
display(df.describe())

# Random sample of 5 rows for inspection
print("\n Sample Data Rows:")
display(df.sample(5))

# Standardize Column Names & Remove Duplicates
df.columns = df.columns.str.strip().str.lower().str.replace(r'[^a-z0-9_]+', '_', regex=True)

# Count and remove exact duplicates
dup_count = df.duplicated().sum()
print(f"\n Found {dup_count} duplicate rows.")
df = df.drop_duplicates()
print(f"After removing duplicates: {df.shape}")

# Detect possible latitude and longitude columns
lat_candidates = [c for c in df.columns if 'lat' in c]
lon_candidates = [c for c in df.columns if 'lon' in c or 'lng' in c]

print(f"Latitude candidates: {lat_candidates}")
print(f"Longitude candidates: {lon_candidates}")

# Convert coordinates to numeric
if lat_candidates and lon_candidates:
    df['latitude_num'] = pd.to_numeric(df[lat_candidates[0]], errors='coerce')
    df['longitude_num'] = pd.to_numeric(df[lon_candidates[0]], errors='coerce')
    df['has_coords'] = df['latitude_num'].notna() & df['longitude_num'].notna()
else:
    # If not found, create placeholder columns
    df['latitude_num'] = np.nan
    df['longitude_num'] = np.nan
    df['has_coords'] = False

# Summary of coordinate availability
valid_coords = df['has_coords'].sum()
print(f"Valid coordinates found for {valid_coords} rows.")
print(f"Invalid or missing coordinates: {len(df) - valid_coords}")

# Clean Text and Categorical Columns
cat_cols = df.select_dtypes(include='object').columns.tolist()

# Strip spaces, replace 'nan'/'None' with NaN
for col in cat_cols:
    df[col] = df[col].astype(str).str.strip().replace({'nan': np.nan, 'None': np.nan, '': np.nan})

# Fill missing small categorical columns with 'Unknown'
for col in cat_cols:
    if df[col].nunique() < 20:
        df[col] = df[col].fillna('Unknown')

# Clean phone/contact columns (keep only digits and '+')
phone_cols = [c for c in df.columns if 'phone' in c or 'contact' in c or 'tel' in c]
for col in phone_cols:
    df[col] = df[col].astype(str).str.replace(r'[^0-9+]', '', regex=True)

print(f"Cleaned {len(phone_cols)} phone/contact columns (if any).")

# Cleaned Dataset
cleaned_path = '/content/health_facility_locations_cleaned.csv'

df.to_csv(cleaned_path, index=False)

print(f"Cleaned dataset saved at: {cleaned_path}")
print(f"Final dataset shape: {df.shape}")

# Visualizations for Insights

plt.style.use('seaborn-v0_8')
sns.set_palette("crest")

# Facility Type Distribution
fac_cols = [c for c in df.columns if 'facility_type' in c or 'fac_type' in c]
if fac_cols:
    fac_col = fac_cols[0]
    plt.figure(figsize=(10,5))
    df[fac_col].value_counts().head(15).plot(kind='bar', color='teal')
    plt.title(f"Top 15 Facility Types ({fac_col})")
    plt.xlabel("Facility Type")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.show()

# Region/District Distribution
dist_cols = [c for c in df.columns if 'district' in c or 'county' in c or 'state' in c]
if dist_cols:
    dist_col = dist_cols[0]
    plt.figure(figsize=(10,5))
    df[dist_col].value_counts().head(15).plot(kind='bar', color='orange')
    plt.title(f"Top 15 Regions by Facility Count ({dist_col})")
    plt.xlabel("Region")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.show()

# Correlation Heatmap
num_df = df.select_dtypes(include=[np.number])
if num_df.shape[1] > 1:
    plt.figure(figsize=(8,6))
    sns.heatmap(num_df.corr(), cmap='viridis', annot=False)
    plt.title("Correlation Heatmap (Numeric Features)")
    plt.show()

# Coordinate Scatter Plot
if df['has_coords'].sum() > 0:
    plt.figure(figsize=(6,6))
    plt.scatter(df['longitude_num'], df['latitude_num'], alpha=0.5, s=10, color='purple')
    plt.title("Facility Locations (Valid Coordinates)")
    plt.xlabel("Longitude")
    plt.ylabel("Latitude")
    plt.show()
else:
    print("No valid coordinates found — skipping map visualization.")

# Install required libraries
!pip install pandas numpy matplotlib seaborn --quiet

# Import essential libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Configure global display options
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', lambda x: f'{x:.3f}')

print("Libraries imported successfully.")

# Load and Preview Dataset
file_path = '/content/health_facility_locations (RAW).csv'
df = pd.read_csv(file_path)

# Show basic info
print(f"Dataset loaded successfully: {df.shape[0]} rows × {df.shape[1]} columns")

# Preview the first few records
display(df.head())

# Display column names
print("\nColumn Names:")
print(df.columns.tolist())

# Check data types
print("\nData Types Summary:")
print(df.dtypes.value_counts())

# Check for missing values
print("\nMissing Values (Top 15):")
print(df.isna().sum().sort_values(ascending=False).head(15))

# Display statistical summary for numeric columns
print("\nNumeric Column Summary:")
display(df.describe())

# Show a random sample
print("\nRandom Sample:")
display(df.sample(5))

# Standardize column names: lowercase and replace spaces/special chars
df.columns = df.columns.str.strip().str.lower().str.replace(r'[^a-z0-9_]+', '_', regex=True)

# Remove exact duplicates
duplicates = df.duplicated().sum()
print(f"Duplicate rows found: {duplicates}")
df = df.drop_duplicates()
print(f"Shape after removing duplicates: {df.shape}")

# Detect potential latitude and longitude columns
lat_cols = [c for c in df.columns if 'lat' in c]
lon_cols = [c for c in df.columns if 'lon' in c or 'lng' in c]

print("Possible latitude columns:", lat_cols)
print("Possible longitude columns:", lon_cols)

# Convert to numeric
if lat_cols and lon_cols:
    df['latitude_num'] = pd.to_numeric(df[lat_cols[0]], errors='coerce')
    df['longitude_num'] = pd.to_numeric(df[lon_cols[0]], errors='coerce')
    df['has_coords'] = df['latitude_num'].notna() & df['longitude_num'].notna()
else:
    df['latitude_num'] = np.nan
    df['longitude_num'] = np.nan
    df['has_coords'] = False

# Summary
print(f"Valid coordinate rows: {df['has_coords'].sum()}")

# Identify categorical columns
cat_cols = df.select_dtypes(include='object').columns

# Trim spaces and replace invalid entries
for col in cat_cols:
    df[col] = df[col].astype(str).str.strip().replace({'nan': np.nan, 'None': np.nan, '': np.nan})

# Fill short categorical columns with 'Unknown'
for col in cat_cols:
    if df[col].nunique() < 20:
        df[col] = df[col].fillna('Unknown')

# Clean phone/contact columns by keeping digits and '+'
phone_cols = [c for c in df.columns if 'phone' in c or 'contact' in c or 'tel' in c]
for col in phone_cols:
    df[col] = df[col].astype(str).str.replace(r'[^0-9+]', '', regex=True)

print("Text and contact information cleaned successfully.")

cleaned_path = '/content/health_facility_locations_cleaned.csv'
df.to_csv(cleaned_path, index=False)

print(f"Cleaned dataset saved at: {cleaned_path}")
print(f"Final dataset shape: {df.shape}")

# UNIVARIATE ANALYSIS

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

plt.style.use('seaborn-v0_8')
sns.set_palette('crest')

print("UNIVARIATE ANALYSIS")

# Distribution of Numeric Columns
num_cols = df.select_dtypes(include=[np.number]).columns

for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(df[col].dropna(), kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

# Countplots for Categorical Columns
cat_cols = df.select_dtypes(include='object').columns

for col in cat_cols:
    if df[col].nunique() <= 20:
        plt.figure(figsize=(7,4))
        sns.countplot(data=df, x=col, order=df[col].value_counts().index)
        plt.title(f"Count of {col}")
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

# Missing Value Percentage
missing = df.isna().mean() * 100
missing = missing[missing > 0].sort_values(ascending=False)

if not missing.empty:
    plt.figure(figsize=(8,5))
    sns.barplot(x=missing.values, y=missing.index, color='salmon')
    plt.title("Percentage of Missing Values per Column")
    plt.xlabel("Missing (%)")
    plt.ylabel("Columns")
    plt.tight_layout()
    plt.show()
else:
    print("No missing values found.")

# Install / import required libraries
!pip install scikit-learn statsmodels --quiet

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Choose numeric columns for regression
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Ensure at least 2 numeric columns exist
if len(num_cols) < 2:
    raise ValueError("Need at least two numeric columns for regression.")

# Inspect numeric columns
print("Numeric Columns Available for Regression:")
print(num_cols)

# SIMPLE LINEAR REGRESSION

from math import sqrt

# Choose dependent (y) and independent (x) variables
y_col = num_cols[0]     # target variable
x_col = num_cols[1]     # single feature

X = df[[x_col]].dropna()
y = df.loc[X.index, y_col]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train model
slr = LinearRegression()
slr.fit(X_train, y_train)

# Predict and evaluate
y_pred = slr.predict(X_test)

# Compute metrics safely
mse = mean_squared_error(y_test, y_pred)
rmse = sqrt(mse)

print(f"Intercept: {slr.intercept_:.3f}")
print(f"Coefficient for {x_col}: {slr.coef_[0]:.3f}")
print(f"R² Score: {r2_score(y_test, y_pred):.3f}")
print(f"RMSE: {rmse:.3f}")

# Plot regression line
plt.figure(figsize=(6,4))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title(f"Simple Linear Regression: {y_col} vs {x_col}")
plt.show()

# Regression line on feature space
plt.figure(figsize=(6,4))
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression Line')
plt.xlabel(x_col)
plt.ylabel(y_col)
plt.legend()
plt.tight_layout()
plt.show()

# MULTIPLE LINEAR REGRESSION
y_col = num_cols[0]
X_cols = num_cols[1:6]  # choose up to 5 numeric predictors

X = df[X_cols].dropna()
y = df.loc[X.index, y_col]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
mlr = LinearRegression()
mlr.fit(X_train, y_train)

# Predict
y_pred = mlr.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
rmse = sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Intercept: {mlr.intercept_:.3f}")
print("\nCoefficients:")
for col, coef in zip(X_cols, mlr.coef_):
    print(f"  {col}: {coef:.3f}")

print(f"\nR² Score: {r2:.3f}")
print(f"RMSE: {rmse:.3f}")

# Plot Actual vs Predicted
plt.figure(figsize=(6,4))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Multiple Linear Regression: Actual vs Predicted")
plt.tight_layout()
plt.show()

# BACKWARD ELIMINATION
import statsmodels.api as sm

X = df[X_cols].dropna()
y = df.loc[X.index, y_col]

# Add intercept term
X_ols = sm.add_constant(X)
model = sm.OLS(y, X_ols).fit()

print(model.summary())

# Backward elimination loop
cols = X_ols.columns.tolist()
p_limit = 0.05
iteration = 1

while True:
    model = sm.OLS(y, X_ols[cols]).fit()
    max_p = model.pvalues.max()
    if max_p > p_limit:
        excluded = model.pvalues.idxmax()
        print(f"Iteration {iteration}: Removing '{excluded}' (p={max_p:.3f})")
        cols.remove(excluded)
        iteration += 1
    else:
        break

print("\nFinal Model Summary (after Backward Elimination):")
final_model = sm.OLS(y, X_ols[cols]).fit()
print(final_model.summary())
print(f"\nRetained Features: {cols}")

# Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, classification_report, confusion_matrix

# Models
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.svm import SVR, SVC
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from xgboost import XGBRegressor, XGBClassifier

df = pd.read_csv("health_facility_locations_cleaned.csv")

# Preview data
print(df.shape)
df.head()

# Handle Categorical Columns
cat_cols = df.select_dtypes(include=['object']).columns

# Label encode categorical variables
encoder = LabelEncoder()
for col in cat_cols:
    df[col] = encoder.fit_transform(df[col].astype(str))

# Split into Features and Target
target_col = 'deemed'
X = df.drop(columns=[target_col])
y = df[target_col]

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.impute import SimpleImputer

# Identify numeric and categorical columns
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.tolist()

# Drop numeric columns that are fully empty (all NaN)
fully_empty = [col for col in num_cols if df[col].isnull().all()]
print("Dropping fully empty numeric columns:", fully_empty)

df.drop(columns=fully_empty, inplace=True)
num_cols = [col for col in num_cols if col not in fully_empty]  # update list

# Impute numeric columns (median)
if len(num_cols) > 0:
    num_imputer = SimpleImputer(strategy='median')
    df[num_cols] = pd.DataFrame(
        num_imputer.fit_transform(df[num_cols]),
        columns=num_cols
    )

# Impute categorical columns (most frequent)
if len(cat_cols) > 0:
    cat_imputer = SimpleImputer(strategy='most_frequent')
    df[cat_cols] = pd.DataFrame(
        cat_imputer.fit_transform(df[cat_cols]),
        columns=cat_cols
    )

print("Remaining NaN values:", df.isnull().sum().sum())

# Handle Categorical Columns
cat_cols = df.select_dtypes(include=['object']).columns

# Label encode categorical variables
encoder = LabelEncoder()
for col in cat_cols:
    df[col] = encoder.fit_transform(df[col].astype(str))

# Split into Features and Target
target_col = 'deemed'
X = df.drop(columns=[target_col])
y = df[target_col]

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Regression Models
regressors = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42, n_estimators=100),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "SVR": SVR(),
    "KNN Regressor": KNeighborsRegressor(),
    "XGBoost": XGBRegressor(random_state=42, eval_metric='rmse')
}

reg_results = []

# Train and evaluate
for name, model in regressors.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    reg_results.append((name, r2, rmse))

# Convert results to DataFrame
reg_results_df = pd.DataFrame(reg_results, columns=["Model", "R² Score", "RMSE"])
reg_results_df.sort_values(by="R² Score", ascending=False, inplace=True)
print("\nRegression Model Results:")
display(reg_results_df)

# Classification Models
if y.nunique() > 10:
    y = pd.cut(y, bins=2, labels=[0, 1])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifiers = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42, n_estimators=100),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "SVM": SVC(),
    "KNN Classifier": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(random_state=42, eval_metric='logloss')
}

class_results = []

for name, model in classifiers.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    class_results.append((name, acc))

# Results DataFrame
class_results_df = pd.DataFrame(class_results, columns=["Model", "Accuracy"])
class_results_df.sort_values(by="Accuracy", ascending=False, inplace=True)
print("\nClassification Model Results:")
display(class_results_df)

# Visualizing Model Performance
plt.figure(figsize=(10,5))
sns.barplot(data=reg_results_df, x="Model", y="R² Score")
plt.title("Regression Model Comparison (R² Scores)")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10,5))
sns.barplot(data=class_results_df, x="Model", y="Accuracy")
plt.title("Classification Model Comparison (Accuracy)")
plt.xticks(rotation=45)
plt.show()

# Cross Validation
model = RandomForestRegressor(random_state=42, n_estimators=100)
scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"Cross-Validation R² Scores: {scores}")
print(f"Average R² Score: {scores.mean():.3f}")

!pip install catboost lightgbm xgboost --quiet

# IMPORT LIBRARIES
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, classification_report

# Regression models
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR

# Classification models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# Boosting Libraries
from xgboost import XGBRegressor, XGBClassifier
from lightgbm import LGBMRegressor, LGBMClassifier
from catboost import CatBoostRegressor, CatBoostClassifier

from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/health_facility_locations (RAW).csv')

# Identify numeric and categorical columns
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.tolist()

# Drop fully empty numeric columns
fully_empty = [col for col in num_cols if df[col].isnull().all()]
df.drop(columns=fully_empty, inplace=True)
num_cols = [col for col in num_cols if col not in fully_empty]

# Impute missing numeric values (median)
if len(num_cols) > 0:
    imputer_num = SimpleImputer(strategy='median')
    df[num_cols] = pd.DataFrame(imputer_num.fit_transform(df[num_cols]), columns=num_cols)

# Impute missing categorical values (most frequent)
if len(cat_cols) > 0:
    imputer_cat = SimpleImputer(strategy='most_frequent')
    df[cat_cols] = pd.DataFrame(imputer_cat.fit_transform(df[cat_cols]), columns=cat_cols)

# Encode categorical variables
encoder = LabelEncoder()
for col in cat_cols:
    df[col] = encoder.fit_transform(df[col].astype(str))

print("Data cleaned successfully.")

# SPLIT FEATURES AND TARGET
target_col = "CAPACITY"
X = df.drop(columns=[target_col])
y = df[target_col]

# Split into train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# REGRESSION MODELS
regressors = {
    "Linear Regression": LinearRegression(),
    "Ridge": Ridge(),
    "Lasso": Lasso(),
    "ElasticNet": ElasticNet(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42, n_estimators=100),
    "Extra Trees": ExtraTreesRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "AdaBoost": AdaBoostRegressor(random_state=42),
    "KNN Regressor": KNeighborsRegressor(),
    "SVR": SVR(),
    "XGBoost": XGBRegressor(random_state=42, eval_metric='rmse'),
    "LightGBM": LGBMRegressor(random_state=42),
    "CatBoost": CatBoostRegressor(verbose=0, random_state=42)
}

reg_results = []

for name, model in regressors.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    reg_results.append((name, r2, rmse))

reg_results_df = pd.DataFrame(reg_results, columns=["Model", "R² Score", "RMSE"]).sort_values(by="R² Score", ascending=False)
print("\n Regression Results:")
display(reg_results_df)

# CLASSIFICATION MODELS
if y.nunique() > 10:
    y = pd.cut(y, bins=2, labels=[0, 1])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifiers = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42, n_estimators=100),
    "Extra Trees": ExtraTreesClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "KNN Classifier": KNeighborsClassifier(),
    "SVM": SVC(),
    "XGBoost": XGBClassifier(random_state=42, eval_metric='logloss'),
    "LightGBM": LGBMClassifier(random_state=42),
    "CatBoost": CatBoostClassifier(verbose=0, random_state=42)
}

class_results = []

for name, model in classifiers.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    class_results.append((name, acc))

class_results_df = pd.DataFrame(class_results, columns=["Model", "Accuracy"]).sort_values(by="Accuracy", ascending=False)
print("\n Classification Results:")
display(class_results_df)

# VISUALIZE MODEL PERFORMANCE
plt.figure(figsize=(10,5))
sns.barplot(data=reg_results_df, x="Model", y="R² Score", palette="Blues_d")
plt.title("Regression Model Comparison (R² Scores)")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10,5))
sns.barplot(data=class_results_df, x="Model", y="Accuracy", palette="Greens_d")
plt.title("Classification Model Comparison (Accuracy)")
plt.xticks(rotation=45)
plt.show()

df.corrwith(df['CAPACITY'])

# K-MEANS CLUSTERING
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Load your cleaned dataset
df = pd.read_csv('/content/cleaned_dataset.csv')

print("Dataset loaded successfully.")
print("Shape:", df.shape)
df.head()

# Select only numeric features
numeric_df = df.select_dtypes(include=['int64', 'float64']).copy()

# Handle missing values
numeric_df.fillna(numeric_df.median(), inplace=True)

# Scale the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_df)

print("Numeric data scaled and ready for clustering.")

# Elbow Method
inertia = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(K, inertia, 'bo-', markersize=6)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

# Silhouette Scores
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(scaled_data)
    score = silhouette_score(scaled_data, labels)
    print(f"k = {k}, Silhouette Score = {score:.4f}")

# Apply K-Means
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(scaled_data)

print(f"K-Means applied successfully with {optimal_k} clusters.")
df['Cluster'].value_counts()

# Reduce dimensions for visualization
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(scaled_data)

plt.figure(figsize=(8,6))
sns.scatterplot(x=reduced_data[:,0], y=reduced_data[:,1],
                hue=df['Cluster'], palette='Set2', s=50)
plt.title(f"K-Means Clustering Visualization (k={optimal_k})")
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.show()

# Cluster centroids in original feature space
centroids = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_),
                         columns=numeric_df.columns)
print("Cluster Centers:")
display(centroids)

# Mean values per cluster
cluster_summary = df.groupby('Cluster')[numeric_df.columns].mean()
print("Cluster Summary (average feature values):")
display(cluster_summary)

# clustered dataset
df.to_csv("health_facility_clusters.csv", index=False)
print("Clustered dataset saved as health_facility_clusters.csv")

# Deep Learning and basic libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Core libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import r2_score, mean_squared_error

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import matplotlib.pyplot as plt

# Load dataset
from google.colab import files
uploaded = files.upload()

df = pd.read_csv('/content/cleaned_dataset.csv')

# Preview data
print("Dataset Shape:", df.shape)
df.head()

# Fill missing values with median or mode
df = df.fillna(df.median(numeric_only=True))

# Select target and features
X = df.drop(columns=['capacity'])
y = df['capacity']

# Encode categorical columns
for col in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define ANN architecture
model = Sequential([
    Dense(128, activation='relu', input_dim=X_train.shape[1]),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)  # Output for regression
])

# Compile model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Predictions
y_pred = model.predict(X_test).flatten()

# Evaluation metrics
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"R² Score: {r2:.3f}")
print(f"RMSE: {rmse:.3f}")

plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
